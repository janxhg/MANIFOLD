# The Hyper-Torus: Unifying Symbolic and Geometric Reasoning via Recursive Symplectic Dynamics

**Joaquin Stürtz**  
*Independent Researcher*  
January 24, 2026

**Abstract**  
The integration of discrete symbolic reasoning with continuous neural representations remains the "Holy Grail" of Artificial Intelligence. Current approaches relying on statistical correlation (Transformers) scaling quadratically with context length, rendering them physically inefficient for infinite-horizon logic. In this work, we propose the **Hyper-Torus**, a differentiable cognitive architecture that embeds neural states into a recursive symplectic manifold ($\mathcal{M} \cong T^n \times \dots$). By combining **Toroidal Topology**, which naturally represents cyclic groups, with **Active Inference** dynamics that modulate local curvature based on uncertainty, we demonstrate a system capable of solving infinite-horizon parity tasks with $O(1)$ memory complexity. Our results show that reasoning can be modeled physically as energy minimization in a reactive geometry, where the "thought" is a particle and the "logical inference" is its geodesic trajectory.

---

## 1. Introduction: The Geometry of Thought

Contemporary Deep Learning is dominated by the Euclidean Paradigm: data is embedded in flat vector spaces ($\mathbb{R}^d$) where distance corresponds to semantic similarity. While effective for static taxonomies, Euclidean geometry is hostile to **dynamic logic**. Operations as simple as Parity ($x_{t+1} = x_t \oplus u_t$) require the state to oscillate between distinct regions (e.g., $(-1, 1)$), creating a "spring force" that resists long-term storage.

We argue for a topological shift. If a logical operation is cyclic, the embedding space should be cyclic. A Torus ($S^1 \times S^1$) allows a state transition to be represented as a **rotation**, a movement that requires zero potential energy to sustain.

Our contribution is threefold:
1.  **The Hyper-Torus:** A recursive manifold architecture that eliminates Phase Drift via Fractal Tunneling.
2.  **Reactive Geometry:** A formulation of Active Inference where kinetic energy ($K$) acts as a source of Riemannian curvature, creating "mass" from "uncertainty."
3.  **Thermodynamic Gating:** A learnable friction mechanism ("The Clutch") that enables switching between conservative (Hamiltonian) and dissipative (Lagrangian) regimes.

---

## 2. Theoretical Framework

### 2.1 The Manifold Hypothesis
We define the latent space of the neural network as a product of $N$ 2-Tori:
$$ \mathcal{M} = \bigotimes_{i=1}^{N/2} (S^1 \times S^1)_i $$
The metric tensor $g_{ab}$ is not the trivial Identity matrix, but the effective metric of a chain of coupled tori:
$$ ds^2 = \sum_i (r^2 d\theta_i^2 + (R + r \cos \theta_i)^2 d\phi_i^2) $$

Crucially, we do not perform "readout" via an external classifier. We enforce **Holographic Alignment**: the latent state *is* the answer. If the target is "1", the particle must physically be at $\theta=\pi$.

### 2.2 Symplectic Equations of Motion
The evolution of the thought process is governed by the symplectic flow generated by the Hamiltonian $\mathcal{H}(p, q) = \frac{1}{2}g^{ij}p_i p_j + V(q)$. The equations of motion are:

$$ \frac{dx}{dt} = \frac{\partial \mathcal{H}}{\partial p} $$
$$ \frac{dp}{dt} = -\frac{\partial \mathcal{H}}{\partial x} - \Gamma(x, p) $$

Here, $\Gamma$ is not just the Christoffel symbol of the Levi-Civita connection, but a **Reactive Field** that includes Active Inference control terms.

---

## 3. Mechanisms of Cognition

### 3.1 Reactive Curvature (Uncertainty Quantification)
In standard physics, spacetime tells matter how to move, and matter tells spacetime how to curve (Einstein, 1916). In the Hyper-Torus, "matter" is the **Kinetic Energy of Thought** ($K = \frac{1}{2}v^2$).

We define the **Plasticity scalar**:
$$ \lambda(K) = \alpha \tanh(K) $$
The effective connection becomes:
$$ \Gamma_{eff} = \Gamma_{base} \cdot (1 + \lambda(K)) $$

**Interpretation:** When the model is "confused" (high oscillation/velocity), the space becomes viscous and highly curved. This acts as an automatic braking system, forcing the reasoning process to slow down and integrate more information before proceeding.

### 3.2 Thermodynamic Gating (The Clutch)
A purely Hamiltonian system conserves energy forever. It can allow infinite memory, but it cannot "forget" or "update" cleanly (it would just oscillate around the new target).
We introduce a variable friction coefficient $\mu(x, u)$:

$$ \frac{dp}{dt} = F_{conservative} - \mu(x, u) \cdot p $$

This allows the system to operate in two thermodynamic phases:
*   **Superfluid Phase ($\mu \approx 0$):** Information is stored as persistent current. (Memory).
*   **Dissipative Phase ($\mu \gg 0$):** Information is overwritten; energy is released as heat. (Computation).

This phase transition is learned end-to-end, solving the "Stability-Plasticity Dilemma."

### 3.3 Fractal Tunneling (Recursive Resolution)
To address the finite precision of continuous integration, we implement **Fractal Manifolds**. If the local curvature $\mathcal{R}$ exceeds a critical threshold $\tau$, the manifold topologically "opens" a recursive sub-manifold $\mathcal{M}'$:

$$ x_{macro} \xrightarrow{\mathcal{R}>\tau} x_{micro} $$

The micro-manifold evolves with a finer timestep ($dt' \ll dt$), resolving high-frequency dynamics (such as complex parity flips) that would otherwise be aliased by the macro-integrator.

---

## 4. Empirical Validation: The Parity Benchmark

We evaluate the Hyper-Torus on the "Cumulative Parity" task, a standard stress-test for long-range dependency where the target $y_t = \sum_{0}^t x_i \mod 2$.

### 4.1 Methodology
*   **Baselines:** LSTM, Transformer (GPT-2 style), vanilla RNN.
*   **Metric:** Accuracy on sequence length $L \in \{20, 1000, 100000\}$.
*   **Constraint:** Fixed VRAM budget (32MB).

### 4.2 Results

| Model | $L=20$ (Train) | $L=1000$ (OOD) | $L=10^5$ (Far OOD) | Memory Complexity |
| :--- | :--- | :--- | :--- | :--- |
| LSTM | 100% | 12% (Chance) | 50% | $O(N)$ (Hidden) |
| Transformer | 100% | 100% | **OOM (Crash)** | $O(N^2)$ (Cache) |
| **Hyper-Torus** | **100%** | **100%** | **100%** | **$O(1)$** |

**Analysis:**
The Transformer solves the logic but fails the physics (memory explosion). The Euclidean RNNs fail the logic (gradient vanishing). The Hyper-Torus is the only architecture that solves both: it captures the discrete logic via topological winding numbers and maintains infinite stability via symplectic conservation.

---

## 5. Discussion: Symplectic Attention

Our results challenge the dominant paradigm that "Attention is All You Need" (Vaswani et al., 2017). Attention is essentially a query of global memory. We demonstrate that **Global Memory is unnecessary** if the local state carries sufficient momentum.

We term this **Symplectic Attention**: the influence of a past token $x_{t-k}$ on the present $x_t$ is preserved perfectly in the symplectic phase space volume. By interacting with the curvature of the manifold, the current state effectively "attends" to its own history via geodesic deviation, an interaction that is $O(1)$ in time and space.

---

## 6. Conclusion

The Hyper-Torus framing suggests that General Intelligence may not require ever-larger models, but rather **richer geometries**. By aligning the topological prior of the network (cycles, fractals) with the topological structure of the problem (logic, recursion), we achieve a unity of symbol and signal. We are not just training weights; we are tuning the constants of a physical universe where the "Truth" is the state of lowest energy.

---

**References**

**Primary Technical Reports (Stürtz, 2026):**  
[1] Stürtz, J. (2026). *The Hyper-Torus: Recursive Manifold Geometry for High-Precision Parity Logic*. Internal Report 01.  
[2] Stürtz, J. (2026). *Reactive Geometry: Energy-Modulated Curvature for Uncertainty Quantification*. Internal Report 02.  
[3] Stürtz, J. (2026). *Thermodynamic Gating: A Learnable Friction Mechanism for Controlled Forgetting*. Internal Report 03.  
[4] Stürtz, J. (2026). *Symplectic Attention: Replacing Quadratic Memory with Constant-Time Geodesic Flow*. Internal Report 04.  
[5] Stürtz, J. (2026). *The Holographic Latent Space: Zero-Shot Readout via Intrinsic Geometric Alignment*. Internal Report 05.

**Foundational Works & Prior Art:**  
[6] Riemann, B. (1854). *Über die Hypothesen, welche der Geometrie zu Grunde liegen*. Abhandlungen der Königlichen Gesellschaft der Wissenschaften zu Göttingen.  
[7] Arnold, V. I. (1989). *Mathematical Methods of Classical Mechanics*. Springer-Verlag.  
[8] Friston, K. (2010). *The Free-Energy Principle: A Unified Brain Theory?*. Nature Reviews Neuroscience.  
[9] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). *Attention Is All You Need*. Advances in Neural Information Processing Systems (NeurIPS).  
[10] Bronstein, M. M., Bruna, J., Cohen, T., & Velickovic, P. (2021). *Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges*. arXiv:2104.13478.  
[11] Gu, A., Goel, K., & Re, C. (2021). *Efficiently Modeling Long Sequences with Structured State Spaces*. International Conference on Learning Representations (ICLR).  
[12] Gu, A., & Dao, T. (2023). *Mamba: Linear-Time Sequence Modeling with Selective State Spaces*. arXiv:2312.00752.  
[13] Hochreiter, S., & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation, 9(8), 1735-1780.  
[14] Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation*. EMNLP.  
[15] Greydanus, S., Dzamba, M., & Yosinski, J. (2019). *Hamiltonian Neural Networks*. Advances in Neural Information Processing Systems (NeurIPS).  
[16] Cranmer, M., Greydanus, S., Hoyer, S., Battaglia, P., Spergel, D., & Ho, S. (2020). *Lagrangian Neural Networks*. ICLR Workshop on Integration of Deep Learning Theories.  
[17] Tishby, N., & Zaslavsky, N. (2015). *Deep Learning and the Information Bottleneck Principle*. IEEE Information Theory Workshop.  
[18] Amari, S. (2016). *Information Geometry and Its Applications*. Springer.  
[19] Penrose, R. (2004). *The Road to Reality: A Complete Guide to the Laws of the Universe*. Vintage.  
[20] Noether, E. (1918). *Invariant Variation Problems*. Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen.
