# Geodesic Flow Networks (GFN): A Physics-Informed Paradigm for Constant-Memory Sequence Modeling

**Author:** Joaquin Stürtz  
**Date:** January 26, 2026

**Abstract**  
We introduce **Geodesic Flow Networks (GFN)**, a novel architecture for sequence modeling that treats computation as the formation of trajectories in a learned latent manifold. Unlike Transformer-based models that require an explicit memory cache (KV-cache) scaling with sequence length, GFN maintains a compact, fixed-size phase state $(x, v)$ and updates it via **geometric integration** of forced geodesic equations. By parameterizing the manifold's curvature and dissipation through learned low-rank Christoffel symbols and state-dependent gating, GFN achieves constant-memory inference and long-horizon stability. We demonstrate that GFN naturally captures cyclic and modular structures when operating on compact topologies like the $n$-torus, providing a physically-grounded alternative to traditional recurrent and attention-based networks.

---

## 1. Introduction

The scaling laws of modern deep learning have prioritized the Transformer architecture, which relies on global attention mechanisms. However, the $O(L^2)$ training complexity and $O(L)$ inference memory (KV-cache) pose fundamental limits for long-horizon reasoning and real-time processing on edge devices.

**Geodesic Flow Networks (GFN)** propose a shift from statistical interpolation to **physical simulation**. In GFN, the latent state of the model is a particle moving through a high-dimensional manifold $\mathcal{M}$. Each input token acts as an external force that redirects this particle. The "memory" of the network is not a static buffer, but the momentum and position of the particle in phase space. This approach guarantees $O(1)$ memory complexity and enables the use of structure-preserving numerical methods (symplectic integrators) to ensure long-term stability.

## 2. Mathematical Framework

### 2.1 The Phase Space Representation
A GFN represents its internal state at time $t$ as a point in the tangent bundle $T\mathcal{M}$ of a $d$-dimensional manifold $\mathcal{M}$:
$$ S_t = (x_t, v_t) \in \mathcal{M} \times T\mathcal{M} $$
where $x_t \in \mathbb{R}^d$ represents the "semantic position" and $v_t \in \mathbb{R}^d$ the "thought velocity."

### 2.2 Continuous-Time Dynamics
The evolution of the state is governed by a forced geodesic equation with dissipation:
1. **Kinematics:** $\dot{x} = v$
2. **Dynamics:** $\dot{v} = F_{\text{ext}}(u) - \Gamma(x, v) - \mu(x, u) \odot v$

Where:
- $F_{\text{ext}}(u)$ is the external force generated by the input token $u$.
- $\Gamma(x, v)$ is the internal geometric resistance (curvature), parameterized by Christoffel symbols.
- $\mu(x, u)$ is the **Dissipation Coefficient** (or "Clutch"), which controls the transition between conservative memory and state rewriting.

### 2.3 Learned Low-Rank Curvature
To keep the parameter count tractable, we decompose the Christoffel symbols using a symmetric low-rank approximation:
$$ \Gamma^k_{ij}(x) \approx \sum_{r=1}^R W_{kr} (U_{ir} \cdot U_{jr}) $$
In practice, the curvature force is computed as:
$$ \Gamma(x, v) = W \cdot \left( \frac{(U^T v)^2}{1 + \|U^T v\|} \right) $$
where $U, W \in \mathbb{R}^{d \times R}$ are learnable parameters. This formulation ensures that the curvature interaction is multiplicative and energy-dependent, mimicking the behavior of non-Euclidean spaces.

## 3. The "Clutch" Mechanism (Thermodynamic Gating)

A critical innovation in GFN is the state-dependent dissipation $\mu(x, u)$. This term acts as a "semantic clutch":
- **Conservative Phase ($\mu \to 0$):** The system behaves like a Hamiltonian system, preserving momentum. This is ideal for long-term memory where information must persist across many tokens.
- **Dissipative Phase ($\mu \to \infty$):** The system becomes over-damped, quickly erasing previous momentum and allowing the new force $F_{\text{ext}}$ to redefine the state. This is used for context switching or logical resets.

The dissipation is implemented via a learned gate:
$$ \mu(x, u) = \sigma(W_f \cdot \phi(x) + W_i \cdot \text{Embed}(u) + b_f) $$
where $\phi(x)$ represents the coordinate mapping (e.g., Fourier features for toroidal topology).

## 4. Geometric Integration

The continuous dynamics are discretized using a family of integrators that preserve the geometric structure of the manifold.

### 4.1 Symplectic Integration (Leapfrog)
For long-horizon stability, we utilize the **Leapfrog** scheme (Kick-Drift-Kick), which is symplectic and preserves phase-space volume:
1. **Kick:** $v_{n+1/2} = \frac{v_n + \frac{\Delta t}{2}(F_{\text{ext}} - \Gamma)}{1 + \frac{\Delta t}{2}\mu}$
2. **Drift:** $x_{n+1} = x_n + \Delta t \cdot v_{n+1/2}$
3. **Kick:** $v_{n+1} = \dots$ (symmetric update at new position)

### 4.2 High-Order Methods
For tasks requiring high precision in smooth regions, GFN supports **Heun (RK2)** and **RK4** methods. However, as identified in the "Runge-Kutta Paradox," lower-order methods or symplectic schemes are preferred when the learned manifold contains logical singularities or sharp transitions.

## 5. Topological Inductive Biases

GFN allows for the explicit selection of manifold topology. The most powerful configuration for symbolic reasoning is the **$n$-Torus** ($\mathbb{T}^n$):
- **Bounded States:** Coordinates are wrapped within $[0, 2\pi)$, preventing numerical divergence.
- **Cyclic Logic:** The torus naturally represents modular arithmetic and periodic structures (e.g., clock-like memory).
- **Metric Continuity:** We use Fourier features $[\sin(x), \cos(x)]$ to ensure that the learned curvature and gating are continuous across the periodic boundaries.

## 6. Multi-Head Geodesic Flow

Depth in GFN is achieved by stacking **Manifold Layers**. Each layer implements a Multi-Head Geodesic Flow:
1. The state $(x, v)$ is split into $H$ independent subspaces (heads).
2. Each head evolves on its own manifold with potentially different topologies (e.g., some heads Euclidean, others Toroidal).
3. The resulting states are concatenated and mixed via a linear projection to enable inter-head communication.

## 7. Computational Efficiency

GFN provides a unique Pareto efficiency in the memory-compute landscape:
- **Inference Memory:** $O(1)$ relative to sequence length. The state size is fixed at $2 \times d$.
- **Training Complexity:** $O(L)$ per sequence, compatible with parallel scan algorithms that reformulate the recurrence as an associative prefix sum in $O(\log L)$ time.
- **Physical Grounding:** The use of $\Delta t$ and physical units (force, mass, friction) allows for better interpretability and control over the model's "thinking speed."

## 8. Conclusion

Geodesic Flow Networks represent a step towards a more robust and efficient architecture for artificial intelligence. By grounding the latent dynamics in Riemannian geometry and classical mechanics, GFN overcomes the memory bottlenecks of Transformers and the instability of traditional RNNs. The framework of learned manifolds, symplectic integration, and topological gating provides a fertile ground for developing models that are not only statistically powerful but physically stable and logically coherent.

---

## References

[1] Arnold, V. I. (1989). *Mathematical Methods of Classical Mechanics*. Springer.  
[2] Hairer, E., et al. (2006). *Geometric Numerical Integration*. Springer.  
[3] Chen, R. T. Q., et al. (2018). *Neural Ordinary Differential Equations*. NeurIPS.  
[4] Marsden, J. E., & West, M. (2001). *Discrete Mechanics and Variational Integrators*. Acta Numerica.  
[5] Strogatz, S. H. (2018). *Nonlinear Dynamics and Chaos*. CRC Press.  
[6] Gu, A., et al. (2021). *Efficiently Modeling Long Sequences with Structured State Spaces*. ICLR.  
[7] Stürtz, J. (2026). *The Runge-Kutta Paradox in Neural Manifolds*. (In preparation).  
