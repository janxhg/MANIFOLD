# MANIFOLD: Technical Handbook
**Version:** 1.0.0
**Status:** Unified Reference

This handbook provides the mathematical foundation, training protocols, and technical specifications for the MANIFOLD architecture.

---

## I. Fundamental Equations

The MANIFOLD architecture is governed by the principles of **Symplectic Sequence Modeling**.

### 1. The Geodesic Equation
State evolution is modeled as a particle moving along the shortest path (geodesic) in a learned Riemannian latent space:

$$ \frac{d^2x^k}{d\tau^2} + \Gamma^k_{ij} \frac{dx^i}{d\tau} \frac{dx^j}{d\tau} = F^k_{ext} $$

Where:
- $x$: The **Semantic Position** (Latent state).
- $v = \frac{dx}{d\tau}$: The **Semantic Velocity** (Information momentum).
- $\Gamma^k_{ij}$: The **Christoffel Symbols**, defining the curvature of the manifold.
- $F_{ext}$: The **External Force**, generated by token embeddings $E(token_t)$.

### 2. Low-Rank Christoffel Parameterization
To ensure $O(d)$ parameter complexity, we approximate the Christoffel symbols using a symmetric low-rank decomposition:

$$ \Gamma(v, x) \approx W \cdot \left[ (U^T v)^2 \odot \text{sigmoid}(V^T x) \right] $$

This formulation allows for $O(d^2)$ interaction resolution (Multi-Head Attention-like mixing) while remaining linear in memory.

### 3. Symplectic Integration (Leapfrog)
To preserve information over infinite horizons, we use a 2nd-order Velocity Verlet scheme:

1.  $v_{t+1/2} = v_t + \frac{1}{2}\Delta t \cdot A(x_t, v_t)$
2.  $x_{t+1} = x_t + \Delta t \cdot v_{t+1/2}$
3.  $v_{t+1} = v_{t+1/2} + \frac{1}{2}\Delta t \cdot A(x_{t+1}, v_{t+1/2})$

This preservation of phase-space volume ensures that gradients neither vanish nor explode.

---

## II. Loss Engine (The Force Fields)

MANIFOLD uses a composite loss function to balance task performance with geometric stability.

### 1. Hamiltonian Loss (`hamiltonian_loss`)
**Purpose**: Energy Conservation.
**Formula**: $L_H = \lambda_h \sum |E_t - E_{t-1}|$ where $E = \|v\|^2$.
**Usage**: Prevents violent transitions in latent energy. If the model "jolts" between tokens, this loss penalizes the discontinuity.
- **High Weight**: Stiff, linear reasoning.
- **Low Weight**: Fluid, highly curved reasoning.

### 2. Geodesic Regularization (`geodesic_regularization`)
**Purpose**: Curvature Control.
**Formula**: $L_G = \lambda_g \| \Gamma(v, v) \|^2$.
**Usage**: Prevents "Semantic Black Holes" (singularities). It keeps the manifold "flat" locally, ensuring that small changes in input don't lead to catastrophic changes in state.

### 3. Curiosity Loss (`curiosity_loss`)
**Purpose**: Entropy Maximization / Exploration.
**Formula**: $L_C = -\lambda_c \sum \log(\text{std}(v))$.
**Usage**: Prevents "Cognitive Collapse" where all neurons synchronize to the same value. It forces the manifold to use its full dimensional capacity.
> [!WARNING]
> High curiosity weights can cause divergence in rigid logic tasks (e.g., Parity).

### 4. Noether Loss (`noether_loss`)
**Purpose**: Semantic Symmetry.
**Usage**: Ensures that different "Heads" in the manifold learn consistent physical laws. It enforces $SO(N)$ rotational invariance in the latent space.

---

## III. Riemannian Optimization

Standard optimizers (Adam, SGD) assume a flat Euclidean space. MANIFOLD requires **Riemannian Optimization** to respect weight constraints.

### 1. The Euclidean Drift Problem
In curved spaces, a standard update $W \leftarrow W - \eta \nabla$ moves weights "off" the intended manifold, leading to "Gradient Drift" and loss oscillations.

### 2. RiemannianAdam
Implementation: `gfn.optim.RiemannianAdam`.
**Key Mechanism: Retraction.**
After the Adam update, we project the weights back to the manifold boundary:

$$ W_{new} = \text{Retract}(W_{old} - \eta \cdot \hat{g}) = \frac{W_{old} - \eta \cdot \hat{g}}{\max(1, \|W\| / \text{max\_norm})} $$

**Recommended Settings**:
- `lr`: $1e-4$ (Stability) to $3e-4$ (Speed).
- `max_norm`: $10.0$ (Stops weight explosion).
- `retraction`: `'normalize'`.

---

## IV. Stability Protocols

1.  **Gradient Clipping**: Strictly use $0.05$ to $0.1$. Manifold dynamics are sensitive to high-frequency noise.
2.  **Velocity Normalization**: The v1.0 engine automatically normalizes $v$ to unit norm post-integration to prevent numerical drift.
3.  **Warm-up**: Always use a learning rate warm-up (e.g., `OneCycleLR`) to allow curvature kernels to stabilize before high-speed training.

---
*For implementation details, see [TRAINING.md](TRAINING.md) or the core [ARCHITECTURE.md](ARCHITECTURE.md).*
