import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import sys
import os
import json
import time
from pathlib import Path
from tqdm import tqdm
from rich.console import Console
from rich.table import Table
from rich.live import Live
from rich.panel import Panel
from rich.layout import Layout
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn

# Add project root
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Import GFN Models & Physics
from gfn.model import Manifold
from gfn.optim import RiemannianAdam
from gfn.losses import geodesic_regularization, hamiltonian_loss, ToroidalDistanceLoss

# Import Baselines & Utils
from tests.benchmarks.baselines import MicroGPT
from tests.benchmarks.bench_utils import ResultsLogger, PerformanceStats

console = Console()

class PeriodicLoss(nn.Module):
    def __init__(self):
        super().__init__()
        
    def forward(self, pred, target):
        return (1.0 - torch.cos(pred - target)).mean()

class ParityTask:
    """Parity Check (Modulo 2) for state tracking."""
    def __init__(self, vocab_size=2, length=20, mod=2):
        self.vocab_size = vocab_size
        self.length = length
        self.mod = mod
        
    def generate_batch(self, batch_size, device='cpu'):
        x = torch.randint(0, self.vocab_size, (batch_size, self.length), device=device)
        y_int = torch.cumsum(x, dim=1) % self.mod
        PI = 3.14159265359
        y_angle = (y_int.float() * 2.0 - 1.0) * (PI * 0.5)
        return x, y_int, y_angle

def train_step_manifold(model, optimizer, scheduler, inputs, targets, targets_class, device):
    optimizer.zero_grad()
    output = model(inputs, collect_christ=False)
    
    if isinstance(output, tuple):
        x_pred = output[0]
    else:
        x_pred = output
        
    y_float = targets.float()
    y_expanded = y_float.unsqueeze(-1).expand_as(x_pred)
    
    criterion = ToroidalDistanceLoss()
    loss_val = criterion(x_pred, y_expanded)
    
    loss_phy = 0.0
    loss_ham = 0.0
    if isinstance(output, tuple) and len(output) >= 6:
        christoffels = output[2]
        v_seq = output[3]
        x_seq = output[4]
        all_forces = output[5]
        
        if christoffels:
            loss_phy = geodesic_regularization(None, christoffels, lambda_g=0.001)
            def first_head_metric(x):
                 return model.layers[0].christoffels[0].get_metric(x) if hasattr(model.layers[0].christoffels[0], 'get_metric') else torch.ones_like(x)
            loss_ham = hamiltonian_loss(v_seq, states=x_seq, metric_fn=first_head_metric, lambda_h=0.0, forces=all_forces)
            
    total_loss = loss_val + loss_phy + loss_ham
    if torch.isnan(total_loss):
        return total_loss, 0.0
        
    total_loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()
    if scheduler: scheduler.step()
    
    with torch.no_grad():
        PI = 3.14159265359
        TWO_PI = 2.0 * PI
        half_pi = PI * 0.5
        dist_pos = torch.min(torch.abs(x_pred - half_pi) % TWO_PI, TWO_PI - (torch.abs(x_pred - half_pi) % TWO_PI))
        dist_neg = torch.min(torch.abs(x_pred + half_pi) % TWO_PI, TWO_PI - (torch.abs(x_pred + half_pi) % TWO_PI))
        d_pos = dist_pos.mean(dim=-1)
        d_neg = dist_neg.mean(dim=-1)
        preds = (d_pos < d_neg).long()
        acc = (preds == targets_class).float().mean().item()
    
    return total_loss.item(), acc

def train_step_gpt(model, optimizer, scheduler, inputs, targets, device):
    optimizer.zero_grad()
    logits = model(inputs)
    criterion = nn.CrossEntropyLoss()
    loss = criterion(logits.view(-1, 2), targets.view(-1))
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
    optimizer.step()
    if scheduler: scheduler.step()
    
    preds = logits.argmax(dim=-1)
    acc = (preds == targets).float().mean().item()
    return loss.item(), acc

def train_model(model_name, model, max_steps=1000, device='cuda'):
    is_manifold = isinstance(model, Manifold)
    if is_manifold:
        optimizer = optim.AdamW([
            {'params': [p for n, p in model.named_parameters() if not any(x in n for x in ['x0', 'v0', 'impulse_scale', 'gate'])], 'lr': 1e-3, 'weight_decay': 1e-4},
            {'params': [p for n, p in model.named_parameters() if any(x in n for x in ['x0', 'v0', 'impulse_scale', 'gate'])], 'lr': 1e-2, 'weight_decay': 0}
        ])
    else:
        optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)

    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-3, total_steps=max_steps, pct_start=0.2)
    model.train()
    
    history = {"loss": [], "acc": []}
    
    pbar = tqdm(range(max_steps), desc=f"Training {model_name}")
    acc_threshold = 0.98
    loss_threshold = 0.2
    min_steps = 100
    patience = 20
    hits = 0
    for i in pbar:
        L = 20
        task = ParityTask(length=L)
        x, y_class, y_angle = task.generate_batch(128, device=device)
        
        if is_manifold:
            loss, acc = train_step_manifold(model, optimizer, scheduler, x, y_angle, y_class, device)
        else:
            loss, acc = train_step_gpt(model, optimizer, scheduler, x, y_class, device)
            
        history["loss"].append(loss)
        history["acc"].append(acc)
        
        if i % 5 == 0:
            pbar.set_postfix({"loss": f"{loss:.4f}", "acc": f"{acc*100:.1f}%"})

        if i >= min_steps and acc >= acc_threshold and loss <= loss_threshold:
            hits += 1
        else:
            hits = 0

        if hits >= patience:
            print(f"\n[GFN] {model_name} converged at step {i}")
            break
                
    return history

def evaluate_scaling(model_name, model, lengths, device='cuda'):
    model.eval()
    results = {"acc": [], "mem": []}
    
    console.print(f"\n[bold yellow][GFN:BENCH][/] Evaluating [cyan]{model_name}[/] Scaling Dynamics...")
    
    table = Table(title=f"Scaling Report: {model_name}")
    table.add_column("Length (N)", justify="right")
    table.add_column("Accuracy", justify="center")
    table.add_column("Peak VRAM", justify="right")

    for L in lengths:
        task = ParityTask(length=L)
        x, y_class, y_angle = task.generate_batch(100, device=device)
        
        def run_inf():
            with torch.no_grad():
                if isinstance(model, Manifold):
                    state = None
                    preds_list = []
                    for t in range(x.shape[1]):
                        out = model(x[:, t:t+1], state=state)
                        l = out[0]
                        state = out[1]
                        PI = 3.14159265359
                        TWO_PI = 2.0 * PI
                        half_pi = PI * 0.5
                        dist_pos = torch.min(torch.abs(l - half_pi) % TWO_PI, TWO_PI - (torch.abs(l - half_pi) % TWO_PI))
                        dist_neg = torch.min(torch.abs(l + half_pi) % TWO_PI, TWO_PI - (torch.abs(l + half_pi) % TWO_PI))
                        d_pos = dist_pos.mean(dim=-1).view(-1)
                        d_neg = dist_neg.mean(dim=-1).view(-1)
                        preds_list.append((d_pos < d_neg).long())
                    return torch.stack(preds_list, dim=1)
                else:
                    return model(x).argmax(dim=-1)

        mem = PerformanceStats.measure_peak_memory(model, run_inf)
        preds = run_inf()
        acc = (preds == y_class).float().mean().item()
        
        results["acc"].append(acc)
        results["mem"].append(mem)
        
        acc_str = f"[bold green]{acc*100:.1f}%[/]" if acc > 0.9 else f"{acc*100:.1f}%"
        table.add_row(str(L), acc_str, f"{mem:.2f} MB")
        torch.cuda.empty_cache()
    
    console.print(table)
    return results

def print_header():
    console.print("\n" + "="*80, style="magenta")
    console.print("  [bold cyan]GFN SUPERIORITY BENCHMARK[/] - [italic]Holographic Manifold vs Transformer[/]", justify="center")
    console.print("="*80, style="magenta")
    console.print(f"  [bold white]Hardware:[/] {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
    console.print(f"  [bold white]Date:[/] {time.ctime()}")
    console.print("="*80 + "\n", style="magenta")

def run_superiority_benchmark():
    logger = ResultsLogger("superiority", category="viz")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print_header()

    dim = 128
    physics_config = {
        'embedding': {'type': 'functional', 'mode': 'linear', 'coord_dim': 16}, 
        'readout': {'type': 'implicit', 'coord_dim': 16},
        'active_inference': {
            'enabled': True, 
            'dynamic_time': {'enabled': True},
            'reactive_curvature': {'enabled': True, 'plasticity': 0.2},
            'singularities': {'enabled': True, 'strength': 20.0, 'threshold': 0.8}
        },
        'fractal': {'enabled': True, 'threshold': 0.5, 'alpha': 0.2},
        'topology': {'type': 'torus'},
        'stability': {'base_dt': 0.4}
    }
    
    manifold = Manifold(vocab_size=2, dim=dim, depth=6, heads=4, integrator_type='leapfrog', physics_config=physics_config, impulse_scale=80.0, holographic=True).to(device)
    gpt = MicroGPT(vocab_size=2, dim=dim, depth=6, heads=1, max_len=100000).to(device)
    
        
    # 2. Training
    h_m = train_model("Manifold-GFN", manifold, max_steps=1000, device=device) # Reduced for quick viz test
    h_g = train_model("Transformer-GPT", gpt, max_steps=1000, device=device)
    
    # 3. Scaling
    lengths = [20, 100, 500, 1000, 2000]
    s_m = evaluate_scaling("Manifold-GFN", manifold, lengths, device)
    s_g = evaluate_scaling("Transformer-GPT", gpt, lengths, device)
    
    # 4. Dashboard Plotting (Cyberpunk Premium Styling)
    sns.set_theme(style="darkgrid", rc={"axes.facecolor": "#121212", "grid.color": "#2a2a2a"})
    plt.rcParams.update({
        'text.color': '#00ADB5',
        'axes.labelcolor': '#00ADB5',
        'xtick.color': '#00ADB5',
        'ytick.color': '#00ADB5',
        'font.family': 'sans-serif',
        'font.weight': 'bold'
    })
    
    fig, axes = plt.subplots(2, 2, figsize=(18, 14), facecolor='#121212')
    plt.subplots_adjust(hspace=0.4, wspace=0.3)
    
    cols = ['#00ADB5', '#FF2E63'] # Cyan and Neon Pink
    
    # Plot A: Convergence
    ax = axes[0, 0]
    ax.plot(h_m["loss"], color=cols[0], label='Manifold GFN (Hamiltonian)', linewidth=2.5)
    ax.plot(h_g["loss"], color=cols[1], label='Transformer (CE)', linewidth=2.5, alpha=0.6)
    ax.set_title("Training Convergence", fontweight='bold', fontsize=18, color='white')
    ax.set_yscale('log')
    ax.legend(facecolor='#1e1e1e', edgecolor=cols[0], labelcolor='white')

    # Plot B: Accuracy
    ax = axes[0, 1]
    ax.plot(np.convolve(h_m["acc"], np.ones(20)/20, mode='valid'), color=cols[0], label='Manifold GFN', linewidth=3.5)
    ax.plot(np.convolve(h_g["acc"], np.ones(20)/20, mode='valid'), color=cols[1], label='Transformer', linewidth=3.5, alpha=0.6)
    ax.set_title("Learning Dynamics", fontweight='bold', fontsize=18, color='white')
    ax.legend(facecolor='#1e1e1e', edgecolor=cols[0], labelcolor='white')

    # Plot C: OOD Generalization
    ax = axes[1, 0]
    ax.plot(lengths, s_m["acc"], 'o-', color=cols[0], label='Manifold GFN', linewidth=5, markersize=12, markerfacecolor='white')
    ax.plot(lengths, s_g["acc"], 's--', color=cols[1], label='Transformer', linewidth=5, markersize=12, alpha=0.6)
    ax.set_title("OOD Stability (Context Scaling)", fontweight='bold', fontsize=18, color='white')
    ax.set_xlabel("Sequence Length")
    ax.set_ylabel("Accuracy")
    ax.legend(facecolor='#1e1e1e', edgecolor=cols[0], labelcolor='white')

    # Plot D: VRAM
    ax = axes[1, 1]
    ax.plot(lengths, s_m["mem"], 'o-', color=cols[0], label='Manifold (Streaming)', linewidth=5, markersize=12, markerfacecolor='white')
    ax.plot(lengths, s_g["mem"], 's--', color=cols[1], label='Transformer (Global)', linewidth=5, markersize=12, alpha=0.6)
    ax.set_title("Memory Constraints", fontweight='bold', fontsize=18, color='white')
    ax.set_yscale('log')
    ax.set_xlabel("Sequence Length")
    ax.set_ylabel("Peak VRAM (MB)")
    ax.legend(facecolor='#1e1e1e', edgecolor=cols[0], labelcolor='white')

    fig.suptitle("GFN vs TRANSFORMER: SUPERIORITY DASHBOARD", fontsize=28, fontweight='bold', y=0.98, color='white')
    logger.save_plot(fig, "gfn_superiority_premium.png")
    
    # FINAL REPORT TABLE
    summary_table = Table(title="[bold yellow]SUPERIORITY SUMMARY[/]", border_style="magenta", show_header=True, header_style="bold cyan")
    summary_table.add_column("Capability", justify="left")
    summary_table.add_column("Manifold-GFN", justify="center")
    summary_table.add_column("Transformer", justify="center")
    summary_table.add_column("Verdict", justify="right")
    
    summary_table.add_row("Long Context (5k)", f"{s_m['acc'][-1]*100:.1f}%", f"{s_g['acc'][-1]*100:.1f}%", "[bold green]GFN[/]" if s_m['acc'][-1] > s_g['acc'][-1] else "Transformer")
    summary_table.add_row("Memory Complexity", "O(1)", "O(NÂ²)", "[bold green]GFN[/]")
    summary_table.add_row("Training Bias", "Hamiltonian", "Empirical", "[bold blue]ISOMORPHIC[/]")
    
    console.print("\n", summary_table)
    console.print("\n[bold green][SUCCESS][/] Benchmark Complete. Dashboard saved to [cyan]results/viz/superiority/gfn_superiority_premium.png[/]\n")

if __name__ == "__main__":
    run_superiority_benchmark()
