model:
  dim: 256
  depth: 6
  heads: 4
  vocab_size: 267735  # WikiText-103 vocab size
  max_len: 512
  use_scan: true
  
physics:
  dt_scale: 0.1
  alpha: 0.99
  solver: "heun"
  active_inference:
    enabled: true
    adaptive_dt: true
    plasticity: true
    
training:
  batch_size: 8
  lr: 2.0e-4
  epochs: 20  # Menos épocas porque el dataset es 50x más grande
  grad_clip: 1.0
  log_every: 100  # Log menos frecuente (más batches)
  save_dir: "checkpoints/wikitext103"
  seed: 42
